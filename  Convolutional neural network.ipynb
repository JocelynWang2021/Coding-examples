{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 4, part 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Fully-connected deep neural networks**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Emilio, Simon** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 21/5** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Xiaofeng Wang, 940326-0228, gusxiaofwa@student.gu.se** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Vasiliki Lamprousi, 880212-3284, gusvasla@student.gu.se** <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical and practical problems should be submitted in this Jupyter notebook.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.  \n",
    "* Your name, personal number and email address should be specified above.\n",
    "\n",
    "**Jupyter/IPython Notebook** is a collaborative Python web-based environment. This will be used in all our Homework Assignments. It is installed in the halls ES61-ES62, E-studio and MT9. You can also use google-colab: https://colab.research.google.com\n",
    "to run these notebooks without having to download, install, or do anything on your own computer other than a browser.\n",
    "Some useful resources:\n",
    "\n",
    "1. https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/ (Quick-start guide)\n",
    "2. https://www.kdnuggets.com/2016/04/top-10-ipython-nb-tutorials.html\n",
    "3. http://data-blog.udacity.com/posts/2016/10/latex-primer/ (latex-primer)\n",
    "4. http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html (markdown)\n",
    "\n",
    "In this assignment you will be using the `pytorch` package. Installation instructions can be found on the [pytorch homepage](https://pytorch.org/get-started/locally/). You don't need the GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Calculating dimensions of output for CNN, 1 point]\n",
    "### 1)\n",
    "Assume you apply a convolutional layer with 8 3x3 filters (stride 1) on a rgb 28x13 image. What will the dimensions of the output be (assuming no padding is done in the convolution)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "The dimensions of the output will be 26x11x8, since the input image size is 28x13x3 (rgb means the number of channel is 3) with 3x3x8 filters and stride 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Counting parameters in a fully connected network, 1 point]\n",
    "### 2)\n",
    "Imagine you apply a two layer fully connected network to a 28x28 rgb image. The hidden layer has dimension 256 and the output is of size 10. How many parameters are necessary? Include the bias parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: <br>\n",
    "Since this is a rgb image, the depth is 3. <br>\n",
    "For each layer, the parameter is 28x28x256, we have 3 layers, so the parameter is 28x28x256x3.<br>\n",
    "For the second layer, the parameter is 256x10, since the hidden layer has dimension 256 and the output is of size 10.<br>\n",
    "The bias parameters for each neurons should be 256+10.<br>\n",
    "The total number of parameters is:<br>\n",
    "28x28x256x3+256x10+256+10 = 604938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Counting parameters in a convolutional network, 1 point]\n",
    "### 3)\n",
    "Apply the following network to the same image, how many parameters are needed? Include bias parameters. Show your calculations.\n",
    "\n",
    "* Convolutional layer with 8 3x3 filters (stride 1).\n",
    "\n",
    "* Max pooling layer (2x2) (stride 2).\n",
    "\n",
    "* Convolutional layer with 16 3x3 filter (stride 1).\n",
    "\n",
    "* Fully connected layer to ouput of size 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "The input size is 28x28x3<br>\n",
    "Conv1 is 3x3x8 and stride is 1, so the output is 26x26x8. The number of parameter in this step is 3x3x8+8 = 80<br>\n",
    "Max pooling layer is 2x2 and stride is 2, so the output is 13x13x8. The number of parameter in this step is 0.<br>\n",
    "Conv2 is 3x3x16 and stride is 1, so the output is 11x11x16. The number of parameter in this step is 3x3x16+16 = 160<br>\n",
    "Fully connected layer to ouput of size 10, so the number of parameter in this step is 11x11x16x10+1 = 19361<br>\n",
    "The total number of parameter is 19361+160+80 = 19601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the amount of parameters for a CNN can be a lot less than for a fully connected network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Applying a filter, 2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image\n",
    "$\n",
    "\\begin{bmatrix}2 & 2 & 1 & 2 \\\\\n",
    "               -2 & -2 & -1 & 1 \\\\\n",
    "               1 & 1 & 2 & 1 \\\\\n",
    "               1 & 1 & 3 & 1 \n",
    "\\end{bmatrix}$\n",
    "Filter \n",
    "$\n",
    "\\begin{bmatrix}1 & 1\n",
    "\\\\-1 & -1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Convolve the filter over the image and apply ReLU, use a stride of 2 with a bias of -2. Try to give an explanation for the output, what is the filter detecting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "Convolve the filter over the image, we get: $\\begin{bmatrix} 8 & 6 & 3\\\\\n",
    "               -6 & -6 & -3\\\\\n",
    "               0 & -1 & -1\n",
    "\\end{bmatrix}$<br><br>\n",
    "Then convolve with 2x2 fliters filled with stride of 2 , we get: $\\begin{bmatrix} 8 & 3\\\\\n",
    "               0 & -1\n",
    "\\end{bmatrix}$<br><br>\n",
    "Next with a bias of -2, we get: $\\begin{bmatrix} 6 & 1\\\\\n",
    "               -2 & -3\n",
    "\\end{bmatrix}$<br><br>\n",
    "Finally apply ReLU, we get: $\\begin{bmatrix} 6 & 1\\\\\n",
    "               0 & 0\n",
    "\\end{bmatrix}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gradients in CNN, 2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply a 2x2 filter (with weights $W$) on a 2x$n$ greyscale image $x$ generating a 1x($n$-1) sequence $z$.\n",
    "\n",
    "We then get \n",
    "$$ \n",
    "\\text{Filter}: \\, \\left[ \\begin{array}{ccc} \n",
    "W_{1,1} & W_{1,2}\\\\\n",
    "W_{2,1} & W_{2,2}\\\\\n",
    "\\end{array}\\right]\n",
    " \\text{Image}: \\, \\left[ \\begin{array}{ccc} \n",
    "x_{1,1} & x_{1,2}  & ... & x_{1,n}\\\\\n",
    "x_{2,1} & x_{2,2}  & ... & x_{2,n} \\\\\n",
    "\\end{array}\\right]\n",
    " \\text{Output}: \\, \\left[ \\begin{array}{ccc} \n",
    "z_{1} & z_{2}  & ... & z_{n-1}\\\\\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, a)\n",
    "Write $z_i$ as a function of $W$ and $x$.\n",
    "<br>Since <br>\n",
    "$\\begin{align}z_1 = \\: W_{1,1}x_{1,1} + W_{1,2}x_{1,2} + W_{2,1}x_{2, 1} + W_{2,2}x_{2,2} \\\\\\end{align}$\n",
    "<br>\n",
    "$\\begin{align}z_2 = \\: W_{1,1}x_{1,2} + W_{1,2}x_{1,3} + W_{2,1}x_{2, 2} + W_{2,2}x_{2,3} \\\\\\end{align}$\n",
    "<br>\n",
    "$\\begin{align}z_3 = \\: W_{1,1}x_{1,3} + W_{1,2}x_{1,4} + W_{2,1}x_{2, 3} + W_{2,2}x_{2,4} \\\\\\end{align}$\n",
    "<br>\n",
    "...<br>\n",
    "We get:<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "z_i = \\: W_{1,1}x_{1,i} + W_{1,2}x_{1,i+1} + W_{2,1}x_{2, i} + W_{2,2}x_{2,i+1} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, b)\n",
    "Lets assume we know $\\frac{dE}{dz_i}$. Calculate the gradient of the error with respect to the filter and the image. You can ignore the edge cases by assuming $x_{i,j}=0$ for i outside the image and similary for $z_i$. \n",
    "<br><br>$\\begin{align} Since \\frac{dz_{i}}{dW_{i,j}} = x_{i,j}, \\ \\frac{dz_{i}}{dx_{i,j}} = W_{i,j}\\ and\\ we\\ know \\frac{dE}{dz_i},\\ we \\ get \\\\ \\end{align}$<br><br>\n",
    "$\\begin{align} \\frac{dE}{dW_{i,j}}= \\: \\frac{dE}{dz_{i}} \\frac{dz_{i}}{dW_{i,j}} = \\frac{dE}{dz_i} x_{i,j}\\\\ \n",
    "\\end{align}$<br><br>\n",
    "$\\begin{align}  \\frac{dE}{dx_{i,j}}= \\:  \\frac{dE}{dz_{i}} \\frac{dz_{i}}{dx_{i,j}} = \\frac{dE}{dz_i} W_{i,j} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both depend on multiple dE/dz. \n",
    "$\\frac{dE}{dW_{i,j}}=\\sum_{k=1}^{n-1} \\frac{dE}{dz_{k}} \\cdot x_{i,j+k-1} $  <br>\n",
    "$\\frac{dE}{dx_{i,j}}=\\frac{dE}{dz_{j-1}} \\cdot W_{i,2} + \\frac{dE}{dz_{j}} \\cdot W_{i, 1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Building a CNN with pytorch, 3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "Build a convolutional network from the following specification:\n",
    "* Input is a single channel 28x28 image (black/white).\n",
    "* Conv1  has 8 3x3 filters with ReLU activation (stride 1)\n",
    "* Max pooling (2x2) layer (stride 2)\n",
    "* Conv2  has 16 3x3 filters with ReLU activation (stride 1)\n",
    "* Reshape data for fully connected layer\n",
    "* Three fully connected layers that will have dimensions 32 and 64 and then 10 for output.\n",
    "    * The first two layers have ReLU activation while the final layer doesn't have any.\n",
    "\n",
    "You will have to figure out some of the implicit dimensions of the different layers and all necessary methods are already imported. You can use tensor.view() to change shape of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, Linear\n",
    "import torch.nn.functional as F\n",
    "#from torch.functional import log_softmax\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU() )\n",
    "        self.fc1   = nn.Linear(11 * 11 * 16, 32)            # with input 28x28x3, our output is 11x11x16\n",
    "        self.fc2   = nn.Linear(32, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Fill in here\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "training_data = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "\n",
    "n = len(training_data)\n",
    "n_train = int(0.9 * n)\n",
    "n_val = n - n_train\n",
    "training_data, validation_data = random_split(training_data, [n_train, n_val])\n",
    "training_loader   = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add optimizer and initialize model\n",
    "import torch\n",
    "model = MyConvNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train network:\n",
    "import torch.nn.functional as F\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for (data, target) in training_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does the network perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 2.00%.\n"
     ]
    }
   ],
   "source": [
    "def validation_error(model):\n",
    "    \"\"\"\n",
    "    Compute the validation error of the given model in percent.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    for (data, target) in validation_loader:\n",
    "        error += torch.sum(model(data).argmax(dim = 1) != target).float()\n",
    "    error /= float(len(validation_data))\n",
    "    return 100.0 * error\n",
    "    \n",
    "print(\"Validation error: {0:.2f}%.\".format(validation_error(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A better classifier, 2 points]\n",
    "Compare with http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\n",
    "\n",
    "Make changes to get a validation error lower than 1.5%. \n",
    "you are allowed (and even encouraged) to use all other available features\n",
    "from the `pytorch` library.\n",
    "Helpful `pytorch` sub-modules are the [nn](https://pytorch.org/docs/stable/nn.html) module and the [optmizers](https://pytorch.org/docs/stable/optim.html) module. \n",
    "\n",
    "Some suggestions on things to try out are:\n",
    "* size of layers\n",
    "* number of layers\n",
    "* learning rate\n",
    "* different optimizer\n",
    "* number of epochs\n",
    "\n",
    "Feel free to be creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, Linear\n",
    "import torch.nn.functional as F\n",
    "#from torch.functional import log_softmax\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyImprovedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyImprovedConvNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))              # here we add Max pooling (2x2) layer (stride 1)\n",
    "        \n",
    "        #we add a new layer: Conv3 has 32 2x2 filters with ReLU activation (stride 1) and Max pooling (2x2) layer (stride 1)\n",
    "        self.conv3 = nn.Sequential(                                  \n",
    "            nn.Conv2d(16, 32, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1   = nn.Linear(8 * 8 * 32, 32)         # with input 28x28x3, our output is 8x8x32\n",
    "        self.fc2   = nn.Linear(32, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Fill in here\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "import torch\n",
    "model_Improved = MyImprovedConvNet()\n",
    "\n",
    "# We use a new optimizer: Adam with learning rate 0.001\n",
    "# From our test, the Validation error is the lowest when learning rate 0.001\n",
    "optimizer_improved = torch.optim.Adam(model_Improved.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train:\n",
    "import torch.nn.functional as F\n",
    "epochs = 10                           # we change the number of epochs to 10\n",
    "for epoch in range(epochs):\n",
    "    for (data, target) in training_loader:\n",
    "        optimizer_improved.zero_grad()\n",
    "        output = model_Improved(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_improved.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 1.43%.\n"
     ]
    }
   ],
   "source": [
    "def validation_error_improved(model):\n",
    "    \"\"\"\n",
    "    Compute the validation error of the given model in percent.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    for (data, target) in validation_loader:\n",
    "        error += torch.sum(model_Improved(data).argmax(dim = 1) != target).float()\n",
    "    error /= float(len(validation_data))\n",
    "    return 100.0 * error\n",
    "    \n",
    "print(\"Validation error: {0:.2f}%.\".format(validation_error_improved(model_Improved)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
